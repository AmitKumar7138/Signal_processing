{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"provenance":[{"file_id":"16J_l2UvHuT1XHThKgutbnIBGefFBRD33","timestamp":1674738850912},{"file_id":"1SkdmruQ3IW-gIZTnl2LRQ_lEDeUq5BBP","timestamp":1630125110295},{"file_id":"1tvYmId8EuAbToBNInaiBWATTs34KcO4x","timestamp":1630090975529},{"file_id":"1efatKbaxKZnwMb9yUxo0jdjYoH2EtA3V","timestamp":1630088567302}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"fj4Sm_eXgGL7"},"source":["Signals from Recordings (Spike Sorting)\n","\n","This implements basic spike sorting techniques on real data from (https://www2.le.ac.uk/centres/csn/software), and simulated data. Problem 1 (50 points) features real data from a single channel. Problem 1 parts will involve implementing (a) bandpass filtering, (b) thresholding, (c) PCA, and (d) K-means. Problem 2 (50 points) will involve applying the (1b-d) to simulated data from multiple channels. Problem 3 (optional for a bonus of 50 points) will also feature the same data as in Problem 2, and will involve the inference of a generative model. Here, rather than assuming putative spikes have been identified and windows around them have been extracted, we'll model the multi-channel voltage time series directly using a convolutional matrix factorization model. We'll use PyTorch to implement the key operations (convolutions and cross-correlations) on a GPU."]},{"cell_type":"markdown","metadata":{"id":"eWrFHo0K9cmf"},"source":["## Instructions\n","Make a copy of this notebook (File $\\rightarrow$ Save a Copy in Drive). Fill in your name above, and any team members you are working with.\n","\n","Complete and run the code cells below to preprocess the data, implement the spike sorting models described in class, and produce some plots.\n","\n","The Problems 1-3 ask you to fill in a few lines of code, denoted as the following\n","```\n","###\n","# This block should do a thing.\n","# YOUR CODE BELOW\n","#\n","\n","result = ...\n","#\n","###\n","```\n","It's ok if you split your answer into more than one line.\n","Don't change the output variable names or the subsequent code won't run!\n","\n","Some problems ask you to discuss the results. Respond to these questions in text cells.\n","\n"]},{"cell_type":"code","metadata":{"id":"GxdSAK6TgGL8"},"source":["import scipy.signal as signal\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from tqdm.auto import trange\n","from scipy.ndimage import gaussian_filter1d\n","from scipy.optimize import linear_sum_assignment\n","\n","# plotting stuff\n","import matplotlib.pyplot as plt\n","from matplotlib.cm import get_cmap\n","from matplotlib.gridspec import GridSpec\n","import seaborn as sns\n","\n","# Enable plots inside the Jupyter Notebook\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOGi4RX8ToVn"},"source":["# Problem 1: Single Channel Electrophysiology Data"]},{"cell_type":"code","metadata":{"id":"sfOh0sHQgPtl"},"source":["# Define data path (this is the same data as in the link above)\n","%%capture\n","!wget -nc https://www.dropbox.com/s/vkjq2nqdyaq8ezf/CSC4.Ncs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VgrQVHLBgQPJ","cellView":"form"},"source":["#@title Function for reading data (run this cell)\n","\n","data_file=\"CSC4.Ncs\"\n","# Header has 16 kilobytes length\n","header_size   = 16 * 1024\n","\n","# Open file\n","fid = open(data_file, 'rb')\n","\n","# Skip header by shifting position by header size\n","fid.seek(header_size)\n","\n","# Read data according to Neuralynx information\n","data_format = np.dtype([('TimeStamp', np.uint64),\n","                        ('ChannelNumber', np.uint32),\n","                        ('SampleFreq', np.uint32),\n","                        ('NumValidSamples', np.uint32),\n","                        ('Samples', np.int16, 512)])\n","\n","raw = np.fromfile(fid, dtype=data_format)\n","\n","# Close file\n","fid.close()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SE8sckg8yvs","cellView":"form"},"source":["#@title Plotting functions (run this cell)\n","\n","def plot_fig(time_, data_, title_='', xlabel_='', ylabel_=''):\n","  fig, ax = plt.subplots(figsize=(12, 4))\n","  ax.plot(time_, data_, 'k')\n","  ax.set_title(title_, fontsize=22)\n","  ax.set_xlim(time_[0], time_[-1])\n","  ax.set_xlabel(xlabel_, fontsize=20)\n","  ax.set_ylabel(ylabel_, fontsize=20)\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eAiFcK8NgGL8"},"source":["## Visualizing the raw data\n","# Get sampling frequency\n","sample_freq = raw['SampleFreq'][0]\n","\n","# Create data vector\n","data = raw['Samples'].ravel()\n","\n","# Determine duration of recording in seconds\n","dur_sec = data.shape[0]/sample_freq\n","print(\"Data is \", dur_sec, \" seconds long\")\n","\n","# Create time vector\n","time = np.linspace(0, dur_sec, data.shape[0])\n","\n","# Plot first second of data\n","plot_fig(time[0:sample_freq], data[0:sample_freq], 'Raw Data', 'time [s]', 'amplitude [uV]')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbsz-m-1gGL9"},"source":["## Problem 1(a): Bandpass filtering\n","First we will bandpass filter each channel from 500Hz to 9kHz to isolate spiking content. We'll use a 10th order Butterworth filter.\n","\n","Use `signal.butter` and `signal.sosfilt` to do this (note that `signal` is an alias for `scipy.signal`; see above). Try calling `help(signal.butter)` or Googling it for more information on the function signature and outputs. The `sample_freq` is specified above."]},{"cell_type":"code","metadata":{"id":"MqtyBeon-0Ef"},"source":["# Construct a Butterworth bandpass filter.\n","order = 10\n","###\n","# Create a Butterworth filter with order 10 from 500Hz to 9kHz using the signal.butter() function\n","#\n","# YOUR CODE BELOW\n","#\n","# sos = signal.butter(...)\n","#\n","###\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKY89qBCAMZt"},"source":["Plot the frequency response of the filter"]},{"cell_type":"code","metadata":{"id":"RUZJ1-r7_0rB"},"source":["# This function evaluates the filter response\n","# at a grid of input frequencies from 0 to the Nyquist frequency\n","# (1/2 the sampling frequency). The response is given\n","# as a complex number for each input frequency, where the square\n","# of the magnitude is the power at that frequency.\n","freqs, response = signal.sosfreqz(sos, fs=sample_freq)\n","\n","# convert the response to decibels and truncate lower end.\n","# (see, e.g., https://en.wikipedia.org/wiki/Decibel)\n","response_db = 20 * np.log10(np.maximum(np.abs(response), 1e-5))\n","\n","# Plot the response.\n","plt.figure(figsize=(8, 6))\n","plt.plot(freqs, response_db, lw=2)\n","plt.vlines([300, 2000], *plt.ylim(), colors='r', ls=':')\n","plt.ylim(-40, 5)\n","plt.grid(True)\n","plt.yticks([0, -20, -40, -60])\n","plt.ylabel('Gain [dB]')\n","plt.title('Butterworth filter frequency response')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5N9s6UyjAYiE"},"source":["Apply the filter to the raw data using `signal.sosfilt`."]},{"cell_type":"code","metadata":{"id":"U1gInS_2AaRF"},"source":["###\n","# Filter the data with the Butterworth filter above, using the signal.sosfilt() function\n","#\n","# YOUR CODE BELOW\n","#\n","# filtered_data = signal.sosfilt(...)\n","#\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fusGEawkA-Tv"},"source":["Plot the raw and bandpass filtered data."]},{"cell_type":"code","metadata":{"id":"6QZ13q-_BB_X"},"source":["plot_fig(time[0:10*sample_freq], data[0:10*sample_freq], 'Raw Data', 'time [s]', 'amplitude [uV]')\n","plot_fig(time[0:10*sample_freq], filtered_data[0:10*sample_freq], 'Bandpass Filtered Data', 'time [s]', 'amplitude [uV]')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xRXsmUJgGMD"},"source":["## Problem 1(b): Extract spikes from the filtered signal\n","Now that we have a clean spike channel we can identify and extract spikes. Let's first normalize the data, then instead of simply thresholding to find the action potentials, we will look for _peaks_ in the data. Check out the [scipy.signal.find_peaks()](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html) function to find out more more."]},{"cell_type":"code","metadata":{"id":"QMwfDkTrJkBC"},"source":["## Normalize the filtered data by computing the z-score before utilizing the find_peaks() function.\n","\n","###\n","# Compute the Z-score of the 'filtered_data' (subtract the mean and divide by the standard deviation)\n","#\n","# YOUR CODE BELOW\n","#\n","# filtered_data_zscore = ...\n","#\n","###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4otTds2iGhHO"},"source":["# Use scipy.signal.find_peaks to find candidate spike times\n","distance_s = 0.005      # time between spikes (in seconds)\n","\n","###\n","# # Pick a number for the minimum threshold that makes sense given the data, i.e., try out several values and see which you think is right.\n","#\n","# YOUR CODE BELOW\n","# min_height = ...  # minimum standard deviation to define a peak, i.e., the peak has to be minimum this height\n","#\n","###\n","max_height = 20     # maximum standard deviation to define a peak, i.e., the peak can be maximum this height, to remove artifacts in the data\n","\n","###\n","# # Find peaks in the z-scored, filtered data such that the minimum height of the peaks is min_height, and maximum is max_height.\n","# # Keep in mind the distance between two consecutive spikes is minimum 'distance_s' (converted to samples).\n","#\n","# YOUR CODE BELOW\n","# spike_inds, spike_heights = signal.find_peaks(...)\n","\n","spike_inds, spike_heights = signal.find_peaks(filtered_data_zscore,  height=[min_height,max_height],  distance=distance_s * sample_freq)\n","#\n","###\n","\n","spike_inds = np.delete(spike_inds, 0) # remove the first spike if too close to time 0\n","\n","# Define waveforms by taking a set number of samples around the peak, and offsetting that slightly\n","num_spikes = len(spike_inds)\n","spike_width = 91 # waveform length, or action potential length\n","offset = 10 # offset the peak of the action potential by this number\n","waveforms = np.zeros((spike_width, num_spikes))\n","for i, ind in enumerate(spike_inds):\n","    window = slice(ind - spike_width // 2 + offset, ind + spike_width // 2 + offset + 1)\n","    waveforms[:,i] = filtered_data_zscore[window]\n","\n","# Delete waveforms with values above max height (artifact)\n","inds_to_del=[]\n","for i, ind in enumerate(spike_inds):\n","  if (waveforms[:,i]).max()>max_height:\n","    inds_to_del.append(i)\n","spike_inds = np.delete(spike_inds, inds_to_del)\n","waveforms = np.delete(waveforms, inds_to_del, axis=1)\n","\n","# plot\n","fig, ax = plt.subplots(figsize=(12, 4))\n","ax.plot(time[:10*sample_freq], filtered_data_zscore[:10*sample_freq], 'k')\n","ax.plot(spike_inds/sample_freq,10*np.ones(spike_inds.shape),'xr')\n","ax.set_title('Detected Spike Times', fontsize=22)\n","ax.set_xlim(time[0], time[10*sample_freq])\n","ax.set_xlabel('time [s]', fontsize=20)\n","ax.set_ylabel('amplitude', fontsize=20)\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhTEFVDBafb1"},"source":["# Visualize the waveforms\n","plot_fig(1000*np.arange(spike_width)/sample_freq, waveforms[:,np.random.randint(0,waveforms.shape[1],30)], 'Waveforms', 'time [ms]', 'amplitude')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RZncATJqh6Cb"},"source":["## Problem 1(c): Dimensionality Reduction\n","\n","We will now implement Principal Component Analysis (PCA) for all the spike waveforms. This will help with the next step of efficient clustering."]},{"cell_type":"code","metadata":{"id":"Et95RZoelav9"},"source":["###\n","# # Implement PCA on the waveforms, with a large number of components, in order to choose how many components we will need to explain the variance in our data\n","#\n","# YOUR CODE BELOW\n","# num_comps = ...\n","# pca = PCA(n_components=num_comps)\n","# pca.fit(...)\n","#\n","###\n","\n","## plot the variance explained ratio, and pick the number of PCs that reach ~90% variance explained\n","plot_fig(np.arange(num_comps)+1,np.cumsum(pca.explained_variance_ratio_),'Variance Explained','Number of Components', 'Fraction of Explained Variance')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-GaXqBAgGME"},"source":["# Choose the number of components to keep, such that ~90% of the variance in the data is explained\n","###\n","# # Perform PCA with the chosen number of components\n","#\n","# YOUR CODE BELOW\n","# num_comps = ...\n","# pca = PCA(n_components=num_comps)\n","# pc_scores = pca.fit_transform(...)\n","##\n","\n","# Plot the first 3 PCs, with the color denoting the 3rd PC.\n","# Remember, each data point here should be a waveform (check that this is true).\n","fig, ax = plt.subplots(figsize=(8, 6))\n","im = ax.scatter(pc_scores[:, 0], pc_scores[:, 1], c=pc_scores[:, 2])\n","cbar = fig.colorbar(im,ax=ax)\n","ax.set_xlabel('1st PC', fontsize=20)\n","ax.set_ylabel('2nd PC', fontsize=20)\n","cbar.set_label('3rd PC', rotation=270)\n","ax.set_title('PCs 1-3', fontsize=23)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7smbaVHzgGME"},"source":["## Problem 1(d) Implement K-means using Lloyd's algorithm\n","\n","Minimize the loss by iterating between finding the cluster means and assigning data points to clusters."]},{"cell_type":"code","metadata":{"id":"-fyVZG3tgGME"},"source":["def k_means(data, num_clusters=3, num_iters=200):\n","    \"\"\"\n","    Input: data of shape [number of samples x number of dimensions], desired number of clusters, and maximum number of iterations\n","    Returns:\n","    1) loss [1x1]\n","    2) cluster assignments [number of samples x 1]\n","    3) cluster_centroids [num_clusters x number of dimensions]\n","    \"\"\"\n","    ###\n","    # # Implement the K-means function such that it takes in the required inputs and produces the variables detailed above.\n","    #\n","    # YOUR CODE BELOW\n","    #\n","    ###\n","    ###\n","    return loss, cluster_assignments, cluster_centroids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3l6zoWXgGME"},"source":["Run the K-means model with different initializations and different number of clusters to determine the 'elbow' in number of clusters."]},{"cell_type":"code","metadata":{"id":"K77SBMi6gGME"},"source":["# This may take a couple of minutes!\n","max_num_clusters = 15\n","num_runs = 10\n","\n","min_loss = []\n","for num_clus in range(1, max_num_clusters +1):\n","    loss_over_runs = []\n","    for run in range(num_runs):\n","        loss, cluster_assignments, cluster_centroids = k_means(pc_scores, num_clus)\n","        loss_over_runs.append(loss)\n","    min_loss.append(np.min(loss_over_runs))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZcNnrtvW5t0b"},"source":["# Plot in order to decide number of clusters by using the elbow method\n","plot_fig(range(1, max_num_clusters +1), min_loss,'Deciding number of clusters', 'Number of Clusters','Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSVJuV14gGMF"},"source":["###\n","# # Implement K-means again with the chosen number of clusters, such that there is an approximate elbow in the curve above\n","#\n","# YOUR CODE BELOW\n","# num_clus = ... # input a number based on the above plot\n","#\n","###\n","\n","loss, cluster_assignments, cluster_centroids = k_means(pc_scores, num_clus)\n","\n","# Plot the result\n","fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n","ax[0].scatter(pc_scores[:, 0], pc_scores[:, 1], c=cluster_assignments)\n","ax[0].plot(cluster_centroids[:,0], cluster_centroids[:,1],'xr')\n","ax[0].set_xlabel('1st principal component', fontsize=20)\n","ax[0].set_ylabel('2nd principal component', fontsize=20)\n","ax[0].set_title('Clustered Data', fontsize=23)\n","\n","time = np.linspace(0, waveforms.shape[0]/sample_freq, waveforms.shape[0])*1000\n","for i in range(num_clus):\n","    cluster_mean = waveforms[:, cluster_assignments==i].mean(axis=1)\n","    cluster_std = waveforms[:, cluster_assignments==i].std(axis=1)\n","\n","    ax[1].plot(time, cluster_mean, label='Cluster {}'.format(i))\n","    ax[1].fill_between(time, cluster_mean-cluster_std, cluster_mean+cluster_std, alpha=0.15)\n","\n","ax[1].set_title('average waveforms', fontsize=23)\n","ax[1].set_xlim([0, time[-1]])\n","ax[1].set_xlabel('time [ms]', fontsize=20)\n","ax[1].set_ylabel('amplitude [uV]', fontsize=20)\n","\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q2VkMSR3yEiJ"},"source":["## Problem 1(e): Discussion\n","This is an open-ended question so there's not necessarily a right answer.  Try to think critically about the algorithm and the results. Please respond to the following prompts:\n","\n","- In practice, you would post-process the extracted spikes to reject unrealistic neurons and merge overlapping ones. How would you approach this problem.\n","- Over the course of a long recording session, the probe could drift up and down so that the channels activated by a neuron shift. How could you compensate for this slow drift in this model, or possibly try to correct for it during preprocessing?\n","\n","*Answer below this line*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"62qmj9lp_tru"},"source":["# Problem 2: Multichannel spike sorting"]},{"cell_type":"code","metadata":{"cellView":"form","id":"ajF4wRhO_x0x"},"source":["#@title Helper function to generate synthetic data (run this cell)\n","\n","def generate_templates(rng, C, D, N):\n","    # Make (semi) random templates\n","    templates = []\n","    for n in range(N):\n","        # center = n * C / (N - 1) if N > 1 else C / 2\n","        center = C * rng.random()\n","        width = 1 + (C / 10) * rng.random()\n","        spatial_factor = np.exp(-0.5 * (np.arange(C) - center)**2 / width**2)\n","\n","        dt = np.arange(D)\n","        period = D / (1 + rng.random())\n","        z = (dt - 0.75 * period) / (.25 * period)\n","        warp = lambda x: -np.exp(-x) + 1\n","        window = np.exp(-0.5 * z**2)\n","        shape = np.sin(2 * np.pi * dt / period)\n","        temporal_factor = warp(window * shape)\n","\n","        template = np.outer(spatial_factor, temporal_factor)\n","        template /= np.linalg.norm(template)\n","        templates.append(template)\n","\n","    return np.array(templates)\n","\n","\n","def generate(rng, T, C, D, N,\n","             mean_amplitude=15,\n","             shape_amplitude=3.0,\n","             noise_std=1,\n","             sample_freq=1000):\n","    \"\"\"Create a random set of model parameters and sample data.\n","\n","    Parameters:\n","    T: integer number of time samples in the data\n","    C: integer number of channels\n","    D: integer duration (number of samples) of each template\n","    N: integer number of neurons\n","    \"\"\"\n","    # Make semi-random templates\n","    templates = generate_templates(rng, C, D, N)\n","\n","    # Make random amplitudes\n","    amplitudes = np.zeros((N, T))\n","    for n in range(N):\n","        num_spikes = rng.poisson(T / sample_freq * 10)\n","        times = rng.integers(0, T, size=num_spikes)\n","        amps = rng.gamma(shape_amplitude,\n","                         scale=mean_amplitude / shape_amplitude,\n","                         size=num_spikes)\n","        amplitudes[n, times] = amps\n","\n","        # Only keep spikes separated by at least D\n","        times, props = signal.find_peaks(amplitudes[n], distance=D, height=1e-3)\n","        amplitudes[n] = 0\n","        amplitudes[n, times] = props['peak_heights']\n","\n","    # Convolve the signal with each row of the multi-channel template\n","    data = 0\n","    for temp, amp in zip(templates, amplitudes):\n","        data += np.row_stack([\n","            np.convolve(amp, row, mode='full')[:-(D-1)]\n","            for row in temp])\n","\n","    data += rng.normal(scale=noise_std, size=data.shape)\n","\n","    return templates, amplitudes, data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"aQeOif5__zyy"},"source":["#@title Helper functions for plotting (run this cell)\n","\n","# initialize a color palette for plotting\n","palette = sns.xkcd_palette([\"windows blue\",\n","                            \"red\",\n","                            \"medium green\",\n","                            \"dusty purple\",\n","                            \"orange\",\n","                            \"amber\",\n","                            \"clay\",\n","                            \"pink\",\n","                            \"greyish\"])\n","sns.set_context(\"notebook\")\n","\n","\n","def plot_data(timestamps,\n","              data,\n","              plot_slice=slice(0, 6000),\n","              labels=None,\n","              spike_times=None,\n","              neuron_channels=None,\n","              spike_width=81,\n","              scale=10,\n","              figsize=(12, 9),\n","              cmap=\"jet\"):\n","    n_channels, n_samples = data.shape\n","    cmap = get_cmap(cmap) if isinstance(cmap, str) else cmap\n","\n","    plt.figure(figsize=figsize)\n","    plt.plot(timestamps[plot_slice],\n","             data.T[plot_slice] - scale * np.arange(n_channels),\n","             '-k', lw=1)\n","\n","    if not any(x is None for x in [labels, spike_times, neuron_channels]):\n","        # Plot the ground truth spikes and assignments\n","        n_units = labels.max()\n","        in_slice = (spike_times >= plot_slice.start) & (spike_times < plot_slice.stop)\n","        labels = labels[in_slice]\n","        times = spike_times[in_slice]\n","        for i in range(n_units):\n","            i_channels = neuron_channels[i]\n","            for t in times[labels == i]:\n","                window = slice(t, t + spike_width)\n","                plt.plot(timestamps[window],\n","                         data.T[window, i_channels] - scale * np.arange(n_channels)[i_channels],\n","                         color=cmap(i / (n_units-1)),\n","                         alpha=0.5,\n","                         lw=2)\n","\n","    plt.yticks(-scale * np.arange(1, n_channels+1, step=2),\n","            np.arange(1, n_channels+1, step=2) + 1)\n","    plt.xlabel(\"time [s]\")\n","    plt.ylabel(\"channel\")\n","    plt.xlim(timestamps[plot_slice.start], timestamps[plot_slice.stop])\n","    plt.ylim(-scale * n_channels, scale)\n","\n","\n","def plot_templates(templates,\n","                   indices,\n","                   scale=0.1,\n","                   n_cols=8,\n","                   panel_height=6,\n","                   panel_width=1.25,\n","                   colors=('k',),\n","                   label=\"neuron\",\n","                   sample_freq=30000,\n","                   fig=None,\n","                   axs=None):\n","    n_subplots = len(indices)\n","    n_cols = min(n_cols, n_subplots)\n","    n_rows = int(np.ceil(n_subplots / n_cols))\n","\n","    if fig is None and axs is None:\n","        fig, axs = plt.subplots(n_rows, n_cols,\n","                                figsize=(panel_width * n_cols, panel_height * n_rows),\n","                                sharex=True, sharey=True)\n","\n","    n_units, n_channels, spike_width = templates.shape\n","    timestamps = np.arange(-spike_width // 2, spike_width//2) / sample_freq\n","    for i, (ind, ax) in enumerate(zip(indices, np.ravel(axs))):\n","        color = colors[i % len(colors)]\n","        ax.plot(timestamps * 1000,\n","                templates[ind].T - scale * np.arange(n_channels),\n","                '-', color=color, lw=1)\n","\n","        ax.set_title(\"{} {:d}\".format(label, ind + 1))\n","        ax.set_xlim(timestamps[0] * 1000, timestamps[-1] * 1000)\n","        ax.set_yticks(-scale * np.arange(n_channels+1, step=4))\n","        ax.set_yticklabels(np.arange(n_channels+1, step=4) + 1)\n","        ax.set_ylim(-scale * n_channels, scale)\n","\n","        if i // n_cols == n_rows - 1:\n","            ax.set_xlabel(\"time [ms]\")\n","        if i % n_cols == 0:\n","            ax.set_ylabel(\"channel\")\n","\n","        # plt.tight_layout(pad=0.1)\n","\n","    # hide the remaining axes\n","    for ax in np.ravel(axs)[n_subplots:]:\n","        ax.set_visible(False)\n","    plt.show()\n","    return fig, axs\n","\n","\n","def plot_model(templates, amplitude, data, scores=None, lw=2, figsize=(12, 6)):\n","    \"\"\"Plot the raw data as well as the underlying signal amplitudes and templates.\n","\n","    amplitude: (N,T) array of underlying signal amplitude\n","    template: (N,C,D) array of template that is convolved with signal\n","    data: (C, T) array (channels x time)\n","    scores: optional (N,T) array of correlations between data and template\n","    \"\"\"\n","    # prepend dimension if data and template are 1d\n","    data = np.atleast_2d(data)\n","    C, T = data.shape\n","    amplitude = np.atleast_2d(amplitude)\n","    N, _ = amplitude.shape\n","    templates = templates.reshape(N, C, -1)\n","    D = templates.shape[-1]\n","    dt = np.arange(D)\n","    if scores is not None:\n","        scores = np.atleast_2d(scores)\n","\n","    # Set up figure with 2x2 grid of panels\n","    fig = plt.figure(figsize=figsize)\n","    gs = GridSpec(2, N + 1, height_ratios=[1, 2], width_ratios=[1] * N + [2 * N])\n","\n","    # plot the templates\n","    t_spc = 1.05 * abs(templates).max()\n","    for n in range(N):\n","        ax = fig.add_subplot(gs[1, n])\n","        ax.plot(dt, templates[n].T - t_spc * np.arange(C),\n","                '-', color=palette[n % len(palette)], lw=lw)\n","        ax.set_xlabel(\"$d$\")\n","        ax.set_xlim([0, D])\n","        ax.set_yticks(-t_spc * np.arange(C))\n","        ax.set_yticklabels([])\n","        ax.set_ylim(-C * t_spc, t_spc)\n","        if n == 0:\n","            ax.set_ylabel(\"channels $c$\")\n","        ax.set_title(\"$W_{{ {} }}$\".format(n+1))\n","\n","    # plot the amplitudes for each neuron\n","    ax = fig.add_subplot(gs[0, -1])\n","    a_spc = 1.05 * abs(amplitude).max()\n","    if scores is not None:\n","        a_spc = max(a_spc, 1.05 * abs(scores).max())\n","\n","    for n in range(N):\n","        ax.plot(amplitude[n] - a_spc * n, '-', color=palette[n % len(palette)], lw=lw)\n","\n","        if scores is not None:\n","            ax.plot(scores[n] - a_spc * n, ':', color=palette[n % len(palette)], lw=lw,\n","                label=\"$y \\star W$\")\n","\n","    ax.set_xlim([0, T])\n","    ax.set_xticklabels([])\n","    ax.set_yticks(-a_spc * np.arange(N))\n","    ax.set_yticklabels([])\n","    ax.set_ylabel(\"neurons $n$\")\n","    ax.set_title(\"amplitude $a$\")\n","    if scores is not None:\n","        ax.legend()\n","\n","    # plot the data\n","    ax = fig.add_subplot(gs[1, -1])\n","    d_spc = 1.05 * abs(data).max()\n","    ax.plot(data.T - d_spc * np.arange(C), '-', color='gray', lw=lw)\n","    ax.set_xlabel(\"time $t$\")\n","    ax.set_xlim([0, T])\n","    ax.set_yticks(-d_spc * np.arange(C))\n","    ax.set_yticklabels([])\n","    ax.set_ylim(-C * d_spc, d_spc)\n","    # ax.set_ylabel(\"channels $c$\")\n","    ax.set_title(\"data $y$\")\n","\n","    # plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGZvY-ua_7V0"},"source":["# Create a larger dataset with a multiple channels and neurons.\n","T = 1000000  # number of time samples\n","C = 10      # number of channels\n","D = 81      # duration of a spike (in samples)\n","N = 5       # multiple neurons\n","\n","# Generate random templates, amplitudes, and noisy data.\n","# `templates` are NxCxD and `amplitudes` are NxT\n","rng = np.random.default_rng(seed=1)\n","print(\"Simulating data. This could take a minute!\")\n","true_templates, true_amplitudes, data = generate(rng, T, C, D, N)\n","plot_model(true_templates, true_amplitudes[:, :2000], data[:,:2000], lw=1, figsize=(12, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fxF87u8KBeov"},"source":["Implement the above steps (Problems 1(b-c)) on the multi-channel data"]},{"cell_type":"markdown","metadata":{"id":"IMuBt88E-m3p"},"source":["## Problem 2(a): Extract putative spikes from Multi-Channel Data"]},{"cell_type":"code","metadata":{"id":"o2IxW-eyAQlR"},"source":["## We first find peaks again to find candidate spike times. However, this time we will look in each channel.\n","# Use scipy.signal.find_peaks to find candidate spike times\n","timestamps = np.arange(T)\n","distance_ms = 0.005  # time between spikes (in seconds)\n","height = 4           # standard deviations to define a peak\n","per_ch_spike_inds = []\n","for ch in trange(C):\n","    ###\n","    # # Implement the 'find_peaks' signal again, this time to find negative peaks (or positive peaks on -data)\n","    #\n","    # YOUR CODE BELOW\n","    #   ch_spike_inds, _ = signal.find_peaks(...)\n","    #\n","    ###\n","    per_ch_spike_inds.append(ch_spike_inds)\n","\n","plot_data(timestamps, data)\n","for ch, ch_spike_inds in enumerate(per_ch_spike_inds):\n","    plt.plot(timestamps[ch_spike_inds],\n","             data[ch, ch_spike_inds] - 10 * ch,\n","             'r.',)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-DCgnuU-uoX"},"source":["Combine nearly coincident spikes across channels\n"]},{"cell_type":"code","metadata":{"id":"lbt8QNFOBYLH"},"source":["# allow a delay of a fraction of the refractory period\n","bins = np.arange(T + 1)\n","###\n","# # Compute the histogram of all the spike times across all channels\n","#\n","# YOUR CODE BELOW\n","#   total_spike_counts, _ = np.histogram(...)\n","#\n","###\n","\n","# Do a little Gaussian smoothing to allow for jitter in spike time across channels\n","jitter_width = 0.0001  # in seconds\n","total_spike_counts = gaussian_filter1d(total_spike_counts.astype(float),\n","                                       jitter_width * sample_freq)\n","\n","\n","###\n","# # # Eyeball a threshold and find peaks\n","# # run with np.inf first and then update with chosen threshold\n","#\n","# YOUR CODE BELOW\n","# min_height = ...\n","\n","min_height = 0.1\n","#\n","###\n","spike_inds, _ = signal.find_peaks(total_spike_counts,\n","                                  height=min_height,\n","                                  distance=0.001 * sample_freq)\n","spike_inds=np.delete(spike_inds,-1) # may need to delete last spike if close to end of time series\n","num_spikes = len(spike_inds)\n","print(\"Found\", num_spikes, \"putative spikes\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PI-uKrH3Dkbr"},"source":["putative_spikes = np.zeros((num_spikes, C, D))\n","for i, ind in enumerate(spike_inds):\n","    window = slice(ind - D // 2, ind + D // 2 + 1)\n","    putative_spikes[i] = data[:, window]\n","\n","# we can use our template plotting code to visualize the spikes too\n","plot_templates(putative_spikes, indices=np.arange(16), scale=10, label=\"spike\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Km1LqNTkII9a"},"source":["## Problem 2(b): Dimensionality Reduction for Multi-Channel Data"]},{"cell_type":"code","metadata":{"id":"4840_gtwEYXk"},"source":["## Flatten the waveform data so that each data point is an entire waveform across all channels.\n","spikes_flat=putative_spikes.reshape((putative_spikes.shape[0],putative_spikes.shape[1]*putative_spikes.shape[2]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"junwzccBH0yD"},"source":["###\n","# # Implement PCA on the waveforms across all channels, with a large number of components,\n","# # Using this plot, we will choose how many components we will need to explain the variance in our data\n","#\n","# YOUR CODE BELOW\n","# num_comps = ...\n","# pca = PCA(n_components=num_comps)\n","# pca.fit(...)\n","#\n","###\n","\n","## plot the variance explained ratio, and pick the number of PCs that reach some saturation in variance explained\n","plt.plot(np.cumsum(pca.explained_variance_ratio_)); plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPC1KiS-IeWV"},"source":["# Choose the number of components to keep, such that variance explained in the data saturates. Question: why does it start saturating so early?\n","###\n","# # Perform PCA with the chosen number of components\n","#\n","# YOUR CODE BELOW\n","# num_comps = ...\n","# pca = PCA(n_components=num_comps)\n","# pc_scores = pca.fit_transform(...)\n","#\n","###\n","\n","# Plot the first 3 PCs, with the color denoting the 3rd PC.\n","# Remember, each data point here is a waveform.\n","fig, ax = plt.subplots(figsize=(8, 6))\n","im = ax.scatter(pc_scores[:, 0], pc_scores[:, 1], c=pc_scores[:, 2])\n","cbar = fig.colorbar(im,ax=ax)\n","ax.set_xlabel('1st PC', fontsize=20)\n","ax.set_ylabel('2nd PC', fontsize=20)\n","cbar.set_label('3rd PC', rotation=270)\n","ax.set_title('PCs 1-3', fontsize=23)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0m-6dEGc-48p"},"source":["## Problem 2(c): Implement K-means for Multi-Channel Data"]},{"cell_type":"markdown","metadata":{"id":"pBcUvSElON1m"},"source":["This time, we run the model with different number of clusters to determine the 'elbow' in number of clusters, but only with one initialization due to the time-consuming nature of the code. This may still take a couple of minutes to run, but will help us decide how many clusters to choose."]},{"cell_type":"code","metadata":{"id":"hu6zAbybON1w"},"source":["max_num_clusters = 15\n","num_runs = 1\n","\n","min_loss = []\n","for num_clus in range(1, max_num_clusters +1):\n","    loss_over_runs = []\n","    for run in range(num_runs):\n","        loss, cluster_assignments, cluster_centroids = k_means(pc_scores, num_clus)\n","        loss_over_runs.append(loss)\n","    min_loss.append(np.min(loss_over_runs))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJ8moaVrON1x"},"source":["# Plot in order to decide number of clusters by using the elbow method\n","plot_fig(range(1, max_num_clusters +1), min_loss,'Deciding number of clusters', 'Number of Clusters','Loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DNknasL5ON1x"},"source":["###\n","# # Implement K-means again with the chosen number of clusters, such that there is an approximate elbow in the curve above\n","# # # Choose the number of clusters based on the above plot.\n","# # # Try different numbers of clusters and continue till the visualization to gain understanding.\n","# YOUR CODE BELOW\n","# num_clus = ...\n","#\n","###\n","loss, cluster_assignments, cluster_centroids = k_means(pc_scores, num_clus)\n","\n","# Plot the result\n","fig, ax = plt.subplots( figsize=(5, 5))\n","ax.scatter(pc_scores[:, 0], pc_scores[:, 1], c=cluster_assignments)\n","ax.plot(cluster_centroids[:,0], cluster_centroids[:,1],'xr')\n","ax.set_xlabel('1st principal component', fontsize=20)\n","ax.set_ylabel('2nd principal component', fontsize=20)\n","ax.set_title('Clustered Data', fontsize=23)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xERE5WNPbfMR"},"source":["## Compute the similarity (correlation coefficient) between the true and inferred templates\n","\n","# Take the cluster centroids and map them back into the original space, and treat each as an individual neuron's template\n","templates = np.reshape(pca.inverse_transform(cluster_centroids),(num_clus,C,D))\n","\n","similarity = np.zeros((N, num_clus))\n","for i in range(N):\n","    for j in range(num_clus):\n","        similarity[i, j] = np.corrcoef(np.reshape(true_templates[i],(-1)), np.reshape(templates[j],-1))[0,1]\n","\n","# Show the similarity matrix\n","_, perm = linear_sum_assignment(similarity, maximize=True)\n","plt.imshow(similarity[:, perm], vmin=0, vmax=1)\n","plt.xlabel(\"true neuron\")\n","plt.ylabel(\"inferred neuron\")\n","plt.title(\"correlation coefficients of amplitudes\")\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8cENd_CP6TC"},"source":["## Plot the true and inferred templates\n","plot_templates(true_templates, np.arange(N), n_cols=N, label='true')\n","plot_templates(templates[perm], np.arange(min(N,num_clus)), n_cols=min(N,num_clus), scale=3, label='matched',colors=('r',))\n","\n","if num_clus>N:\n","    plot_templates(templates[np.setdiff1d(np.arange(num_clus), perm)],\n","                   np.arange(num_clus-N), n_cols=num_clus-N, scale=3, label='others',colors=('r',))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zhw4KCTh_H6o"},"source":["## Problem 2(d): Discussion\n","\n","- How do the recovered templates seem in comparison to the true templates, especially for num_clus $\\geq$ N?\n","- What steps can you take to improve the results?\n","\n","*Answer below this line*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"tB4RL1-hVZt4"},"source":["# Problem 3: Modeling the Voltage Data with Generative Modeling\n","\n","## Problem 3 intro: Convolution and cross-correlation with PyTorch\n","\n","PyTorch is a machine learning framework for implementing fast numerical computations on CPU and GPU hardware. It has a domain specific language that is very similar to NumPy's. Instead of numpy `ndarray`'s, we operate on PyTorch `Tensor`'s. Tensors are stored on the specified device, and in the setup above you'll see that we specified our device to be `'cuda'`, i.e. a GPU. That's also why you need to run this Colab notebook with a GPU Runtime. (If you click the RAM/disk icon in the upper right, you should see that this session is a GPU session. If it's not, go to \"Runtime -> Change Runtime Type\" to select a GPU.)\n","\n","PyTorch `Tensor`s offer a similar interface to Numpy arrays. You can read all about them in the [docs](https://pytorch.org/docs/stable/tensors.html). In this notebook, we'll name our variables with `_t` postscripts to indicate that they are `Tensor`'s. We can convert back and forth between an `array`s and `Tensor`s using the convenience functions `to_t` and `from_t`, which we defined above. (There are a few minor concerns in making the translation; for example, Numpy defaults to 64-bit floats whereas PyTorch defaults to 32-bit. Similarly, we have to make sure that we copy the tensor back to the CPU and \"detach\" it from the computation graph before converting it to a Numpy array.)\n","\n","`Tensor` objects have a few nice functions that will make your life easier in this homework. Suppose `data_t` is a `Tensor`. Then,\n","- `data_t.flip(dims=(-1,))` flips the tensor along its last axis.\n","- `data_t.unsqueeze(0)` creates a new leading axis.\n","- `data_t.permute(1, 0, 2)` permutes the order of the axes.\n","- `data_t.reshape(1, 1, -1)` vectorizes the data and reshapes it to have two leading dimensions of length 1.\n","- `data_t.sum()` sums the entries in the data.\n","\n","If this is your first time using PyTorch, you might want to check out some of their [tutorials](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) first.\n"]},{"cell_type":"code","metadata":{"id":"wsrNL83uXPRA"},"source":["# We'll use PyTorch for this problem\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.distributions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aELwbRYbXS_D"},"source":["# specify that we want our tensors on the GPU and in float32\n","device = torch.device('cuda')\n","dtype = torch.float32\n","\n","# helper function to convert between numpy arrays and tensors\n","to_t = lambda array: torch.tensor(array, device=device, dtype=dtype)\n","from_t = lambda tensor: tensor.to(\"cpu\").detach().numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gEZMH3hCMP24"},"source":["### Problem 3 intro (a): Perform a 1d convolution\n","\n","As a first step, we generate synthetic data simulating the presence of a spike in a single channel. Based on a template and a 1d array representing amplitudes, we simulate data as the convolution of these two arrays.\n","\n","You'll use the `conv1d` function in the `torch.nn.functional` package. We've already imported as `F` so that you can call it with `F.conv1d(...)`. Take a look at its documentation [here](https://pytorch.org/docs/stable/nn.functional.html?highlight=conv1d#torch.nn.functional.conv1d), as well as the corresponding documentation for the `torch.nn.Conv1d` object, which implements a convolutional layer for a neural network.\n","\n","Here's an example,\n","```\n","# make the input (i.e. the signal)\n","B = 1   # batch size\n","N = 2   # number of input channels\n","T = 100 # length of input signal\n","input_t = torch.rand(B, N, T)\n","\n","# make the weights (i.e. the filter)\n","C = 3   # number of output channels\n","D = 10  # length of the filter\n","weight_t = torch.rand(C, N, D)\n","\n","# perform the convolution\n","output_t = F.conv1d(input_t, weight_t)\n","\n","# output.shape is (B, C, T - D + 1)\n","```\n","**Remember that `conv1d` actually performs a cross-correlation!**\n","\n","Let $X \\in \\mathbb{R}^{B \\times N \\times T}$ denote the signal/input and $W \\in \\mathbb{R}^{C \\times N \\times D}$ denote the filter/weights, and let $Y \\in \\mathbb{R}^{B \\times C \\times (T - D + 1)}$ denote the output. Then the `conv1d` function implements the cross-correlation,\n","\\begin{align}\n","y_{b,c,t} = \\sum_{n = 1}^{N} \\sum_{d=1}^D x_{b,n,t+d-1} w_{c,n,d}.\n","\\end{align}\n","for $b=1,\\ldots,B$, $c=1,\\ldots,C$, and $t=1,\\ldots,T-D+1$.\n","\n","By default the output only contains the \"valid\" portion of the convolution; i.e. the $T-D+1$ samples where the inputs and weights completely overlap. If you want the \"full\" output, you have to call `F.conv1d(input_t, weights_t, padding=D-1)`. This pads the input with $D-1$ zeros at the beginning and end so that the resulting output is length $T + D - 1$. Depending your application, you may want the first $T$ or the last $T$ entries in this array. When in doubt, try both and see!\n","\n","Use `conv1d` to implement a 1d **convolution**. Remember that you can do it by cross-correlation as long as you flip your weights along the last axis."]},{"cell_type":"code","metadata":{"id":"_MDZg1AVrXBx"},"source":["# Create a dataset with a single channel and one neuron.\n","T = 1000    # number of time samples\n","N = 1       # one neuron\n","C = 1       # number of channels\n","D = 51      # duration of a spike (in samples)\n","\n","# Generate random templates, amplitudes, and noisy data.\n","# `templates` are NxCxD and `amplitudes` are NxT\n","rng = np.random.default_rng(seed=2)\n","templates, amplitudes, _ = generate(rng, T, C, D, N)\n","\n","# First we'll perform the convolution using numpy.\n","data = np.convolve(amplitudes[0], templates[0, 0], mode='full')[:T]\n","assert data.shape[0] == T\n","\n","# Plot the templates, amplitude, and data\n","plot_model(templates, amplitudes, data)\n","\n","###\n","# Now perform the same convolution using PyTorch's `conv1d` function. F\n","#\n","# YOUR CODE BELOW\n","amplitudes_t = to_t(amplitudes)\n","templates_t = to_t(templates)\n","\n","# data_t = F.conv1d(...)\n","#\n","###\n","\n","assert np.allclose(data, from_t(data_t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGsHSSO6MeR2"},"source":["### Problem 3 intro (b): Perform a 1d cross-correlation in PyTorch\n","\n","Recall from class that the cross-correlation measures the similarity between the template and the actual data at every time window. In those time points where the data and the template coincide, we should obtain a high correlation indicating the presence of a spike. The peaks of the amplitude array and the cross-correlation array should match, as you see in the plot below.  The dotted line shows the cross-correlation of the data and the template, and we see that it peaks where there are spikes in the true underlying amplitude that generated the data. Use `conv1d` to implement this **cross-correlation** and get the dotted line."]},{"cell_type":"code","metadata":{"id":"bsJKzLTUxXUx"},"source":["# Now correlate the data with the template to estimate spike times\n","score = np.correlate(data, templates[0, 0], mode='full')[D-1:]\n","assert score.shape[0] == T\n","\n","plot_model(templates, amplitudes, data, scores=score)\n","\n","###\n","# Now perform the same cross-correlation using PyTorch's `conv1d` function.\n","#\n","# YOUR CODE BELOW\n","#\n","# score_t = F.conv1d(...)\n","#\n","###\n","\n","# Move the data tensor to the CPU, detach it, and convert to a numpy array\n","assert np.allclose(score, from_t(score_t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uPW3qbjpMlqu"},"source":["### Problem 3 intro (c): Perform a 1d convolution across multiple channels at once\n","\n","Similar to problem 3a, except that the template (and therefore the final synthetic data) has multiple _output_ channels."]},{"cell_type":"code","metadata":{"id":"Z56Mvq42T4Zo"},"source":["# Create a dataset with a single channel and one neuron.\n","T = 1000    # number of time samples\n","C = 10      # number of channels\n","D = 100     # duration of a spike (in samples)\n","N = 1       # one neuron\n","\n","# Generate random templates, amplitudes, and noisy data.\n","# `templates` are NxCxD and `amplitudes` are NxT\n","rng = np.random.default_rng(seed=0)\n","templates, amplitudes, _ = generate(rng, T, C, D, N)\n","\n","# Convolve the signal with each row of the multi-channel tempalte\n","data = np.row_stack([\n","    np.convolve(amplitudes[0], row, mode='full')[:T]\n","    for row in templates[0]])\n","\n","plot_model(templates, amplitudes, data)\n","\n","###\n","# Now perform the same convolution using PyTorch's `conv1d` function.\n","#\n","# YOUR CODE BELOW\n","#\n","amplitudes_t = to_t(amplitudes)\n","templates_t = to_t(templates)\n","# data_t = F.conv1d(...)\n","#\n","###\n","\n","assert np.allclose(data, from_t(data_t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4axdluyAMvma"},"source":["### Problem intro 3 (d): Perform a 1d cross-correlation across multiple channels at once\n","\n","Same as Problem 3b except that the data and templates have multiple _input_ channels."]},{"cell_type":"code","metadata":{"id":"5wWa7SpZz2Ds"},"source":["# We'll first perform the cross-correlation in numpy by correlating\n","# each row of the data with the corresponding row of the template and summing.\n","# Then you'll do the same thing in PyTorch using a single call to `F.conv1d`.\n","score = np.sum([np.correlate(data[c], templates[0, c], mode='full')[D-1:]\n","                for c in range(C)], axis=0)\n","\n","plot_model(templates, amplitudes, data, scores=score)\n","\n","###\n","# Now perform the same cross-correlation using PyTorch's\n","# ``nn.functional.conv1d` function. You should only need\n","# one call to this function!\n","#\n","# YOUR CODE BELOW\n","# score_t = F.conv1d(...)\n","#\n","###\n","\n","assert np.allclose(score, from_t(score_t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1dlmZoM8G1Q5"},"source":["### Problem 3 intro (e): Convolving multiple neurons' spikes and templates\n","\n","Similar to problem 3c, but here we have multiple neurons (input channels), each associated to a template with multiple (output) channels. The final simulated data is aggregated across neurons to simulate actual measurements where signals from multiple neurons are superimposed."]},{"cell_type":"code","metadata":{"id":"z5GWnW2WG9cD"},"source":["# Create a dataset with a multiple channels and neurons.\n","T = 1000    # number of time samples\n","C = 10      # number of channels\n","D = 100     # duration of a spike (in samples)\n","N = 3       # multiple neuron\n","\n","# Generate random templates, amplitudes, and noisy data.\n","# `templates` are NxCxD and `amplitudes` are NxT\n","rng = np.random.default_rng(seed=0)\n","templates, amplitudes, _ = generate(rng, T, C, D, N)\n","\n","# Convolve the signal with each row of the multi-channel template\n","data = 0\n","for temp, amp in zip(templates, amplitudes):\n","    data += np.row_stack([\n","        np.convolve(amp, row, mode='full')[:T]\n","        for row in temp])\n","\n","plot_model(templates, amplitudes, data)\n","\n","###\n","# Now perform the convolution using PyTorch's `conv1d` function.\n","# One call to `F.conv1d` should perform the sum over neurons for you.\n","#\n","# YOUR CODE BELOW\n","#\n","templates_t = to_t(templates)\n","amplitudes_t = to_t(amplitudes)\n","# data_t = F.conv1d(...)\n","\n","# permute the channels (out) and neurons (in) dimensions of the templates,\n","# and flip along the duration dimension.\n","#\n","###\n","assert np.allclose(data, from_t(data_t))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4X2UQxJRK2R"},"source":["### Problem 3 intro (f): Perform a 1d cross-correlation across multiple channels and neurons at once\n","\n","Same as Problem 3c but now we're performing the cross-correlation with multiple neurons' templates at once."]},{"cell_type":"code","metadata":{"id":"mBFZBiG7Qe-m"},"source":["# We'll first perform the cross-correlation in numpy by correlating\n","# each row of the data with the corresponding row of each template and summing.\n","# Then you'll do the same thing in PyTorch using a single call to `F.conv1d`.\n","score = np.array([\n","    np.sum([np.correlate(data[c], templates[n, c], mode='full')[D-1:]\n","            for c in range(C)], axis=0)\n","    for n in range(N)])\n","\n","plot_model(templates, amplitudes, data, scores=score)\n","\n","###\n","# Now perform the convolution using PyTorch's `conv1d` function.\n","# One call to `F.conv1d` should perform all cross-correlations for you.\n","#\n","# YOUR CODE BELOW\n","# score_t = F.conv1d(...)\n","#\n","#\n","###\n","\n","assert np.allclose(score, from_t(score_t), atol=1e-4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4k4b9_wja5TP"},"source":["# Problem 3 Model: Modeling the Voltage Data with Generative Modeling\n","\n","We will use the same simulated data as in Problem 2. Make sure you use the same parameters as in Problem 2, with the same seed."]},{"cell_type":"code","metadata":{"id":"aIr73V3-heVn"},"source":["# Create a larger dataset with a multiple channels and neurons.\n","T = 1000000  # number of time samples\n","C = 10      # number of channels\n","D = 81      # duration of a spike (in samples)\n","N = 5       # multiple neurons\n","\n","# Generate random templates, amplitudes, and noisy data.\n","# `templates` are NxCxD and `amplitudes` are NxT\n","rng = np.random.default_rng(seed=1)\n","print(\"Simulating data. This could take a minute!\")\n","true_templates, true_amplitudes, data = generate(rng, T, C, D, N)\n","plot_model(true_templates, true_amplitudes[:, :2000], data[:,:2000], lw=1, figsize=(12, 8))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hegaIsqtRAqX"},"source":["# Generate a set of random templates and amplitudes to seed the model\n","rng = np.random.default_rng(seed=1)\n","templates = generate_templates(rng, C, D, N)\n","amplitudes = np.zeros((N, T))\n","noise_std = 1.0\n","\n","# copy to the device\n","templates_t = to_t(templates)\n","amplitudes_t = to_t(amplitudes)\n","data_t = to_t(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gCwPHmZOrLOS"},"source":["## Problem 3(a): Compute the log likelihood\n","\n","One of the most awesome features of PyTorch is its `torch.distributions` package. See the docs [here](https://pytorch.org/docs/stable/distributions.html). It contains objects for many of our favorite distributions, and has convenient functions for computing log probabilities (with `d.log_prob()` where `d` is a `Distribution` object), sampling (`d.sample()`), computing the entropy (`d.entropy()`), etc. These functions broadcast as you'd expect (unlike `scipy.stats`!), and they're designed to work with automatic differentiation.  More on that another day...\n","\n","For now, you'll use `torch.distributions.Normal` to compute the log likelihood of the data given the template and amplitudes, $\\log p(Y \\mid A, W)$.  To do that, you'll convolve the amplitudes and templates (recall Problem 3 intro (e)) to get the mean value of $Y$, then you'll use the `log_prob` function to evaluate the likelihood of the data."]},{"cell_type":"code","metadata":{"id":"m14LDrF1bWua"},"source":["def log_likelihood(templates_t, amplitudes_t, data_t, noise_std):\n","    \"\"\"Evaluate the log likelihood\"\"\"\n","    N, C, D = templates_t.shape\n","    _, T = data_t.shape\n","\n","    ###\n","    # Compute the log probability\n","    #\n","    # YOUR CODE BELOW\n","    #\n","    # compute the model predictions by convolving the amplitude and templates\n","    # pred_t = F.conv1d(...)\n","    #\n","    # evaluate the log probability using torch.distributions.Normal\n","    # lp = ...\n","    #\n","    ###\n","\n","    # return the log probability normalized by the data size\n","    return lp / (C * T)\n","\n","ll = log_likelihood(templates_t, amplitudes_t, data_t, noise_std)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ULZKN48jsT8y"},"source":["## Problem 3(b): Compute the residual\n","\n","Next, compute the residual for a specified neuron by subtracting the convolved amplitudes and templates for all the other neurons. Again, recall Problem 3 intro (e)."]},{"cell_type":"code","metadata":{"id":"OcPl7gUsNEiN"},"source":["def compute_residual(neuron, templates_t, amplitudes_t, data_t):\n","    N, C, D = templates_t.shape\n","\n","    ###\n","    # Compute the predicted value of the data by\n","    # convolving the amplitudes and the templates for all\n","    # neurons except the specified one.\n","    #\n","    # YOUR CODE BELOW\n","    not_n = np.concatenate([np.arange(neuron), np.arange(neuron+1, N)])\n","    # pred_t = F.conv1d(...)\n","    #\n","    ###\n","\n","    # return the data minus the predicted value given other neurons\n","    return data_t - pred_t\n","\n","residual_t = compute_residual(0, templates_t, amplitudes_t, data_t)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7HWVckFesZAU"},"source":["## Problem 3(c): Compute the score\n","\n","We defined the \"score\" for neuron $n$ to be the cross-correlation of the residual and its template. Compute it using `conv1d`. Recall Problem 3 intro (d)."]},{"cell_type":"code","metadata":{"id":"p2Ww9dNLTscP"},"source":["def compute_score(neuron, templates_t, amplitudes_t, data_t):\n","    N, C, D = templates_t.shape\n","    T = data_t.shape[1]\n","\n","    # first get the residual\n","    residual_t = compute_residual(neuron, templates_t, amplitudes_t, data_t)\n","\n","    ###\n","    # Compute the 'score' by cross-correlating the residual\n","    # and the template for this neuron.\n","    #\n","    # YOUR CODE BELOW\n","\n","    # score_t = F.conv1d(...)\n","    #\n","    ###\n","    return score_t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DM3nt9QysfEA"},"source":["## Problem 3(d): Update the amplitudes using `find_peaks`\n","\n","Our next step is to update the amplitudes given the scores. We'll use the simple heuristic described in lecture to find peaks in the score that are separated by a distance of at least $D$ samples and at least a height of $\\sigma^2 \\lambda$, where $\\sigma$ is the standard deviation of the noise and $\\lambda$ is the amplitude rate hyperparameter. Use the `find_peaks` function from 1(b) to do this, and then update the amplitude tensor with your results"]},{"cell_type":"code","metadata":{"id":"qWscC7mXPJEM"},"source":["def _update_amplitude(neuron, templates_t, amplitudes_t, data_t, noise_std=1.0, amp_rate=5.0):\n","    N, C, D = templates_t.shape\n","    T = data_t.shape[1]\n","\n","    # compute the score and convert it to a numpy array.\n","    score_t = compute_score(neuron, templates_t, amplitudes_t, data_t)\n","    score = from_t(score_t)\n","\n","    ###\n","    # Find the peaks in the cross-correlation and update the amplitude tensor.\n","    #\n","    # YOUR CODE BELOW\n","    # peaks, props = find_peaks(...)\n","    ###\n","\n","    heights = props['peak_heights']\n","\n","    # Update the amplitude tensor for this neuron.\n","    amplitudes_t[neuron] = 0\n","    amplitudes_t[neuron, peaks] = to_t(props['peak_heights'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8EDCljEus1L7"},"source":["## Problem 3(e): Update the templates\n","Our last step is to update the template for a given neuron by projecting $\\overline{R}_n \\in \\mathbb{R}^{C \\times D}$, the sum of scaled residuals at the times of spikes in the amplitudes:\n","\\begin{align}\n","    \\overline{R}_n = \\sum_{t:a_{nt} > 0} a_{nt} R_{n,:,t:t+D}.\n","\\end{align}\n","where $R_n \\in \\mathbb{R}^{C \\times T}$ denotes the residual for neuron $n$.\n","\n","In lecture we suggested a simple trick to implement this summation. First compute a matrix of regressors $X_n \\in \\mathbb{R}^{D \\times T}$ where the $d$-th row is equal to the lagged amplitudes. That is,\n","\\begin{align}\n","x_{ndt} = a_{n,t-d+1}.\n","\\end{align}\n","We can equivalently compute the regressor matrix by convolving the amplitudes with a \"delay\" matrix, which is just the $D \\times D$ identity matrix.\n","\n","Once we have the regressors, we can compute sum of scaled residuals as $\\overline{R}_n = R_n X_n^\\top$.\n","\n","Finally, to get the template, project $\\overline{R}_n$ onto $\\mathcal{S}_K$, the set of rank-$K$, unit-norm matrices, using the SVD."]},{"cell_type":"code","metadata":{"id":"Zbvv-XzndvcE"},"source":["def _update_template(rng, neuron, templates_t, amplitudes_t, data_t, template_rank=1):\n","    N, C, D = templates.shape\n","    T = data_t.shape[1]\n","\n","\n","    # check if the factor is used. if not, generate a random new one.\n","    if amplitudes_t[neuron].sum() < 1:\n","        target_t = to_t(generate_templates(rng, C, D, 1)[0])\n","\n","    else:\n","        ###\n","        # Make a TxD array of regressors for this neuron\n","        # by convolving its amplitude with a \"delay\" matrix;\n","        # i.e. a DxD identity matrix.\n","        #\n","        # YOUR CODE BELOW\n","        delay_t = to_t(np.eye(D))\n","\n","        # regressors_t = F.conv1d(...)\n","        #\n","        ###\n","\n","        # get the residual using the function you wrote above\n","        residual_t = compute_residual(neuron, templates_t, amplitudes_t, data_t)\n","\n","        # compute the target (inner product of residual and regressors)\n","        target_t = residual_t @ regressors_t.T\n","\n","    ###\n","    # Project the target onto the set of normalized rank-K templates using\n","    # `torch.svd` and `torch.norm`. Note that `torch.svd` returns V rather\n","    # than V^T, as `np.linalg.svd` does.\n","    #\n","    # YOUR CODE BELOW\n","    # ...\n","    # templates_t[neuron] = ...\n","    #\n","    ###\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ve_ubfWPJnaK"},"source":["## Put it all together\n","\n","That's it! We've written a little function to perform coordinate ascent using your `_update_*` functions. It tracks the log likelihood at each iteration. (We're ignoring the priors for now). It also uses some nice progress bars so you can see how fast (or slow?) your code runs."]},{"cell_type":"code","metadata":{"id":"BPMbKOV7uUY6"},"source":["def map_estimate(rng,\n","                 templates_t,\n","                 amplitudes_t,\n","                 data_t,\n","                 num_iters=20,\n","                 template_rank=1,\n","                 noise_std=1.0,\n","                 amp_rate=5.0,\n","                 tol=1e-4):\n","    \"\"\"Fit the templates and amplitudes by maximum a posteriori (MAP) estimation.\n","    \"\"\"\n","    N, C, D = templates_t.shape\n","\n","    # make a fancy reusable progress bar for the inner loops over neurons.\n","    outer_pbar = trange(num_iters)\n","    inner_pbar = trange(N)\n","    inner_pbar.set_description(\"updating neurons\")\n","\n","    # track log likelihoods over iterations\n","    lls = [from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std))]\n","    for itr in outer_pbar:\n","        inner_pbar.reset()\n","        for n in range(N):\n","            # update the amplitude\n","            _update_amplitude(n, templates_t, amplitudes_t, data_t, noise_std=noise_std, amp_rate=amp_rate)\n","            # update the template\n","            _update_template(rng, n, templates_t, amplitudes_t, data_t, template_rank=template_rank)\n","            inner_pbar.update()\n","\n","        # compute the log likelihood\n","        lls.append(from_t(log_likelihood(templates_t, amplitudes_t, data_t, noise_std=noise_std)))\n","\n","        # check for convergence\n","        if abs(lls[-1] - lls[-2]) < tol:\n","            print(\"Convergence detected!\")\n","            break\n","\n","    return np.array(lls)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rvtQmiv-ZXQ"},"source":["## Fit the synthetic data and plot the log likelihoods"]},{"cell_type":"code","metadata":{"id":"z817fJGULIkd"},"source":["# Make random templates and set amplitude to zero\n","rng = np.random.default_rng(seed=1)\n","templates = generate_templates(rng, C, D, N)\n","amplitudes = np.zeros((N, T))\n","noise_std = 1.0     # \\sigma\n","amp_rate = 5.0      # \\lambda\n","\n","# copy to the device\n","templates_t = to_t(templates)\n","amplitudes_t = to_t(amplitudes)\n","data_t = to_t(data)\n","\n","# Fit the model.\n","lls = map_estimate(rng, templates_t, amplitudes_t, data_t, noise_std=noise_std, amp_rate=amp_rate)\n","\n","# For comparison, compute the log likelihood with the true templates and amplitudes.\n","true_ll = from_t(log_likelihood(to_t(true_templates),\n","                                to_t(true_amplitudes),\n","                                data_t,\n","                                noise_std))\n","\n","# Plot the log likelihoods\n","plt.plot(lls, '-o')\n","plt.hlines(true_ll, 0, len(lls) - 1, colors='r', linestyles=':', label=\"true LL\")\n","plt.xlabel(\"Iteration\")\n","plt.xlim(-.1, len(lls) - .9)\n","plt.ylabel(\"Log Likelihood\")\n","plt.grid(True)\n","plt.legend(loc=\"lower right\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ethJ1Ya6_dLk"},"source":["## Find a permutation of the inferred neurons that best matches the true neurons"]},{"cell_type":"code","metadata":{"id":"p7Ydy6W7-Mgt"},"source":["# compute the similarity (inner product) of the true and inferred templates\n","templates = from_t(templates_t)\n","similarity = np.zeros((N, N))\n","for i in range(N):\n","    for j in range(N):\n","        similarity[i, j] = np.sum(true_templates[i] * templates[j])\n","\n","# Show the similarity matrix\n","_, perm = linear_sum_assignment(similarity, maximize=True)\n","plt.imshow(similarity[:, perm], vmin=0, vmax=1)\n","plt.xlabel(\"true neuron\")\n","plt.ylabel(\"inferred neuron\")\n","plt.title(\"cosine similarity of amplitudes\")\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zIJqkZBh-dPF"},"source":["## Plot the true and inferred templates\n","\n","They should line up pretty well."]},{"cell_type":"code","metadata":{"id":"CwLzxeJt4sPE"},"source":["# Plot the true and inferred templates, permuted to best match\n","plot_templates(true_templates, np.arange(N), n_cols=N)\n","plot_templates(templates[perm], np.arange(N), n_cols=N, colors=('r',))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RG2VzJjJ__u6"},"source":["## Problem 3(f): Discussion\n","\n","- How does this compare to the clustering (K-means) approach?\n","\n","- What are the advantages of utilizing this kind of model?\n","\n","*Answer below this line*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"VW3jlTAcA7Ms"},"source":["# Submission Instructions\n","- Print to PDF and download an .ipynb file.\n","- Submit both on Canvas (one per team)."]}]}